{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparação dos Documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import chromadb\n",
    "import uuid\n",
    "import os\n",
    "import openai\n",
    "import joblib\n",
    "import numpy as np\n",
    "import nest_asyncio  # noqa: E402\n",
    "from dotenv import load_dotenv\n",
    "from llama_parse import LlamaParse\n",
    "from openai import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_unstructured import UnstructuredLoader\n",
    "from langchain_chroma.vectorstores import Chroma\n",
    "from getpass import getpass\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE = 2000\n",
    "OFFSET = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_client = OpenAI(api_key= os.environ[\"OPENAI_API_KEY\"])\n",
    "# openai_client = OpenAI(api_key= openai_key)\n",
    "# openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Conexão com o cliente do banco de dados ChromaDB\n",
    "chromadb_path = \"G:/Drives compartilhados/RISCO E COMPLIANCE/Relatórios de Risco/Risco/FIDCS/SCRIPTS_RISCO/Projeto IA/Base ChromaDB/\"\n",
    "chroma_client = chromadb.PersistentClient(path= chromadb_path)\n",
    "collection    = chroma_client.get_or_create_collection(name= \"Regulamentos_FIDC_PLN_LLAMA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checa todo o conteúdo inserido no ChromaDB\n",
    "collection.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_or_parse_data():\n",
    "\n",
    "    data_file = \"G:/Drives compartilhados/RISCO E COMPLIANCE/Relatórios de Risco/Risco/FIDCS/SCRIPTS_RISCO/Projeto IA/Base ChromaDB/llama/ICRED INSS - PARSED.pkl\"\n",
    "\n",
    "    if os.path.exists(data_file):\n",
    "        # Load the parsed data from the file\n",
    "        parsed_data = joblib.load(data_file)\n",
    "\n",
    "    else:\n",
    "\n",
    "        # Perform the parsing step and store the result in llama_parse_documents\n",
    "        parsingInstructionFIDC = \"\"\"The provided document is an investment fund regulation.\n",
    "        The document may contain multiple attachments that should be considered as different segments.\n",
    "        It includes many definitions, which may be contained within tables.\n",
    "        Tables may be split into multiple pages, but should be considered as an unique table.\n",
    "        \"\"\"\n",
    "        \n",
    "        parser = LlamaParse(api_key= os.environ[\"LLAMA_API_KEY\"],\n",
    "                            result_type=\"markdown\",\n",
    "                            parsing_instruction= parsingInstructionFIDC,\n",
    "                            page_separator = \"\\n== {pageNumber} ==\\n\",\n",
    "                            #language = \"pt\",\n",
    "                            max_timeout= 5000,)\n",
    "        \n",
    "        llama_parse_documents = parser.load_data(\"G:/Drives compartilhados/GESTAO/_Operacional/Planilhas Gestão/Scripts/temp/temp_regulamentos/ICRED INSS RESP LIMITADA FIDC 1.pdf\")\n",
    "\n",
    "        # Save the parsed data to a file\n",
    "        print(\"Saving the parse results in .pkl format ..........\")\n",
    "        joblib.dump(llama_parse_documents, data_file)\n",
    "\n",
    "        # Set the parsed data to the variable\n",
    "        parsed_data = llama_parse_documents\n",
    "\n",
    "    return parsed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chamada da API OpenAI para transformação do texto em vetor utilizando text-embedding\n",
    "def get_embedding(text):\n",
    "\n",
    "    embedding = OpenAIEmbeddings(\n",
    "        model= \"text-embedding-ada-002\",\n",
    "        chunk_size= CHUNK_SIZE\n",
    "    )\n",
    "\n",
    "    emb = embedding.embed_query(text)\n",
    "\n",
    "    return emb\n",
    "\n",
    "def create_vector_database():\n",
    "\n",
    "    \"\"\"\n",
    "    Creates a vector database using document loaders and embeddings.\n",
    "\n",
    "    This function loads urls, splits the loaded documents into chunks, \n",
    "    transforms them into embeddings using ada,\n",
    "    and finally persists the embeddings into a Chroma vector database.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Call the function to either load or parse the data\n",
    "    llama_parse_documents = load_or_parse_data()\n",
    "    print(llama_parse_documents[0].text[:300])\n",
    "\n",
    "    # Open the file in append mode ('a')\n",
    "    with open(\"G:/Drives compartilhados/RISCO E COMPLIANCE/Relatórios de Risco/Risco/FIDCS/SCRIPTS_RISCO/Projeto IA/Base ChromaDB/llama/output.md\", 'a') as f:  \n",
    "        for doc in llama_parse_documents:\n",
    "            f.write(doc.text + '\\n')\n",
    "\n",
    "    markdown_path = \"G:/Drives compartilhados/RISCO E COMPLIANCE/Relatórios de Risco/Risco/FIDCS/SCRIPTS_RISCO/Projeto IA/Base ChromaDB/llama/output.md\"\n",
    "    loader = UnstructuredLoader(markdown_path)\n",
    "\n",
    "    #loader = DirectoryLoader('data/', glob=\"**/*.md\", show_progress=True)\n",
    "    documents = loader.load()\n",
    "    \n",
    "    # Split loaded documents into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size= CHUNK_SIZE, chunk_overlap=20)\n",
    "    docs = text_splitter.split_documents(documents)\n",
    "\n",
    "    #len(docs)\n",
    "    print(f\"length of documents loaded: {len(documents)}\")\n",
    "    print(f\"total number of document chunks generated :{len(docs)}\")\n",
    "    #docs[0]\n",
    "\n",
    "    # Initialize Embeddings\n",
    "    embed_model = FastEmbedEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\")\n",
    "    \n",
    "    # Create and persist a Chroma vector database from the chunked documents\n",
    "    vs = Chroma.from_documents(\n",
    "        documents= docs,\n",
    "        embedding= embed_model,\n",
    "        persist_directory= \"G:/Drives compartilhados/RISCO E COMPLIANCE/Relatórios de Risco/Risco/FIDCS/SCRIPTS_RISCO/Projeto IA/Base ChromaDB/\",  # Local mode with in-memory storage only\n",
    "        collection_name= \"Regulamentos_FIDC_PLN_LLAMA\"\n",
    "    )\n",
    "\n",
    "    #query it\n",
    "    #query = \"what is the agend of Financial Statements for 2022 ?\"\n",
    "    #found_doc = qdrant.similarity_search(query, k=3)\n",
    "    #print(found_doc[0][:100])\n",
    "    #print(qdrant.get())\n",
    "\n",
    "    print('Vector DB created successfully !')\n",
    "    return vs,embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs,embed_model = create_vector_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checa todo o conteúdo inserido no ChromaDB\n",
    "collection.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementação da RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma(embedding_function=embed_model,\n",
    "                    persist_directory=\"chroma_db_llamaparse1\",\n",
    "                    collection_name=\"rag\")\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={'k': 3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aplicação do LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"Fale sobre o MOVE BRASIL FIAGRO FIDC 1. Cite os participantes de sua estrutura \n",
    "(administrador, gestor, cedente, etc), discorra a respeito dos tipos\n",
    "de direitos creditórios que podem ser adquiridos, quais são os eventos de avaliação do fundo\n",
    "e como é paga a remuneração de gestora e administradora. Também inclua qualquer outro fator\n",
    "relevante para a estrutura do fundo.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_custom_prompt(custom_prompt_template):\n",
    "    \"\"\"\n",
    "    Prompt template for QA retrieval for each vectorstore\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate(template= custom_prompt_template,\n",
    "                            input_variables=['context', 'question'])\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "modelo = ChatOpenAI(model = \"gpt-4o-mini\", temperature= 0, max_tokens= 4096)\n",
    "\n",
    "custom_prompt_template  = \"\"\"Você é um assistente de IA que responde as dúvidas dos usuários com bases nos documentos abaixo.\n",
    "Os documentos abaixo apresentam as fontes atualizadas e devem ser consideradas como verdade. Sempre que uma\n",
    "pergunta oferecer o nome de um FIDC, você deve procurar pela melhor correspondência deste nome na lista de documentos.\n",
    "Se uma informação não for encontrada nos documentos, diga que não foi encontrada. Não tente gerar qualquer definição.\n",
    "Documentos: {documents}\n",
    "\"\"\" #Caso o nome citado na pergunta não exista na lista de documentos, responda que ele não foi encontrado.\n",
    "\n",
    "prompt = set_custom_prompt(custom_prompt_template)\n",
    "\n",
    "messages=[\n",
    "  SystemMessage(content= custom_prompt_template),\n",
    "  HumanMessage(content= question)\n",
    "]\n",
    "\n",
    "prompt = set_custom_prompt()\n",
    "\n",
    "########################### RESPONSE ###########################\n",
    "PromptTemplate(input_variables=['context', 'question'], template=\"Use the following pieces of information to answer the user's question.\\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\nContext: {context}\\nQuestion: {question}\\n\\nOnly return the helpful answer below and nothing else.\\nHelpful answer:\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(llm= modelo,\n",
    "                               chain_type=\"stuff\",\n",
    "                               retriever=retriever,\n",
    "                               return_source_documents=True,\n",
    "                               chain_type_kwargs={\"prompt\": prompt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = modelo.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer.content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
